--- 
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 3, Spring 2019'
output:
  pdf_document:
    toc: no
    toc_depth: '2'
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Mikal Solberg Stapnes'
header-includes: \usepackage{float}
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
library(ggplot2)
library(coda)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE, fig.pos = 'H')
```

# Problem A: Comparing `AR(2)` parameter estimators using resampling of residuals

In this exercise we want to analyse a non-Gaussian time-series, and compare two different parameter estimators. We consider and AR(2) model, which is specified by the relation
$$x_t = \beta_xx_{t-1}+\beta_2x_{t-2}+e_t,$$
where $e_t$ are i.i.d. random variables with zero mean and constant variance. The least sum of squared residuals (LS) and least sum of absolute residuals (LA) are obtained by minimising the following loss functions w.r.t $\boldsymbol \beta$
\begin{align*}
Q_{LS}(\mathbf{x}) &= \sum_{t=3}^T(x_t-\beta_1x_{t-1}-\beta_2x_{t-2})^2,\\
Q_{LA}(\mathbf{x}) &= \sum_{t=3}^T|x_t-\beta_1x_{t-1}-\beta_2x_{t-2}|.
\end{align*}
We denote the minimisers by $\hat{\boldsymbol \beta}_{LS}$ and $\hat{\boldsymbol \beta}_{LA}$, and define the estimated residuals to be $\hat{e}_t= x_t-\hat{\beta}_1x_{t-1}-\hat{\beta}_2x_{t-2}$ for $t=3,\dots,T$, and let $\bar{e}$ be the mean of these. Then we re-center the residuals to have mean zero by defining $\hat{\epsilon}_t = \hat{e}_t - \bar{e}$.
```{r, echo = F, eval = T}
source("probAdata.R")
source("probAhelp.R")
```
## 1.
We now use the residual resampling bootstrap method to evaluate the relative performance of the two parameter estimators. First, we calculate the two estimators by using the provided function `ARp.beta.est()`. Then, we calculate the observed residuals by using the provided function `ARp.resid()`.
```{r, echo = T, eval = T}
# Calculate the two estimators
n = length(data3A$x)
estimators = ARp.beta.est(data3A$x,2)
beta.LS = estimators$LS
beta.LA = estimators$LA

# Calculate observed residuals
e.observed.LS = ARp.resid(data3A$x,beta.LS)
e.observed.LA = ARp.resid(data3A$x,beta.LA)
```

Then, we want to estimate the variance and bias of the two estimators. We use 1500 bootstrap samples, each as long as the original data sequence. To do resampling, we initialise values for $x_1$ and $x_2$ by picking a random consecutive sequence from the data. Then, we use the provided function `ARp.filter()` to generate a new sample based on the bootstrapped residuals. Finally, we regress the new time-series to obtain bootstrapped estimates of the different $\boldsymbol \beta$'s.
```{r, echo = T, eval = T}
set.seed(4300)
# Number of Bootstrap Samples
B = 1500

# Bootstrap the residuals B times
e.bootstrapped = matrix(sample(e.observed.LS, size=B*(n-2), replace=T), nrow = B, ncol = n-2)

# Create a random consecutive sequence to initialise
index = sample(99, B, replace=T)
index.mat = matrix(c(index,index+1),nrow = B,ncol = 2)
x0 = matrix(data3A$x[index.mat], nrow = B, ncol = 2)

# Prepare to allocate data
bootstrapped.LS = matrix(NA, nrow = B, ncol = 2)
bootstrapped.LA = matrix(NA, nrow = B, ncol = 2)
# Prepare to allocate data to create residuals from the B different pairs of beta
e.bootstrapped.beta.LS = matrix(NA, nrow = B, ncol = n-2)
e.bootstrapped.beta.LA = matrix(NA, nrow = B, ncol = n-2)

for (i in 1:B){
  # Create time-series
  bootstrapped.sample.LS = ARp.filter(x0[i,], beta.LS, e.bootstrapped[i,])
  bootstrapped.sample.LA = ARp.filter(x0[i,], beta.LA, e.bootstrapped[i,])
  
  # Regress on the time-series
  bootstrapped.LS[i,] = ARp.beta.est(bootstrapped.sample.LS, 2)$LS
  bootstrapped.LA[i,] = ARp.beta.est(bootstrapped.sample.LA, 2)$LA
  
  # Compute the corresponding residuals based on the created time-series and the computed betas
  e.bootstrapped.beta.LS[i,] = ARp.resid(bootstrapped.sample.LS,bootstrapped.LS[i,])
  e.bootstrapped.beta.LA[i,] = ARp.resid(bootstrapped.sample.LA,bootstrapped.LA[i,])
}
```

We use the bootstrap to obtain variance and bias of the two estimators. To estimate the bias, we use the plug-in principle, and define the estimate of the bias as
$$\text{bias}_{\hat{F}} = E_{\hat{F}} [s(\mathbf{x}^\ast)]-\hat{\boldsymbol \beta},$$
where $\hat{F}$ is the bootstrap sample distribution, $\mathbf{x}^\ast$ is a bootstrap sample and $s(\cdot)$ is the bootstrap estimator. To estimate the variance, we use the sample variance of the bootstrap estimators.

```{r, echo = T, eval = F}
# Mean of bootstrap estimators
bootstrap.estimate.LS = apply(bootstrapped.LS,2,mean)
bootstrap.estimate.LA = apply(bootstrapped.LA,2,mean)

# Variance of bootstrap estimators
bootstrap.variance.LS = apply(bootstrapped.LS,2,var)
bootstrap.variance.LA = apply(bootstrapped.LA,2,var)
cat("Bias of beta.LS: ", bootstrap.estimate.LS - beta.LS, "\n")
cat("Bias of beta.LA: ", bootstrap.estimate.LA - beta.LA, "\n")

cat("Variance of beta.LS: ", bootstrap.variance.LS, "\n")
cat("Variance of beta.LA: ", bootstrap.variance.LA, "\n")
```
```{r, echo = F, eval = T}
# Mean of bootstrap estimators
bootstrap.estimate.LS = apply(bootstrapped.LS,2,mean)
bootstrap.estimate.LA = apply(bootstrapped.LA,2,mean)

# Variance of bootstrap estimators
bootstrap.variance.LS = apply(bootstrapped.LS,2,var)
bootstrap.variance.LA = apply(bootstrapped.LA,2,var)
cat("Bias of beta.LS: ", bootstrap.estimate.LS - beta.LS, "\n")
cat("Bias of beta.LA: ", bootstrap.estimate.LA - beta.LA, "\n")

cat("Variance of beta.LS: ", bootstrap.variance.LS, "\n")
cat("Variance of beta.LA: ", bootstrap.variance.LA, "\n")
```

LS < LA?? EU < NA?

## 2.
We now want to compute a $95\%$ prediction interval for $x_{101}$ based on both the LS and the LA estimator. We use the bootstrapped time-series and the 1500 estimates for $\boldsymbol \beta$ obtained in part 1. to estimate the residual distribution, then we use the following formula to simulate a value $x_{101}$ for the observed time-series
$$x_{101} = \beta_1 x_{99} + \beta_2 x_{100} + e_{101},$$
where $e_{101}$ is drawn from the residual distribution. By doing this, the simulated $x_{101}$ values will reflect both our lack of knowledge about the parameter values, and our lack of knowledge about the residual distribution. We then display histograms of the simulated distributions of $x_{101}$, and also find limits in the prediction interval as quantiles in the simulated values.

```{r, echo = T, eval = F, out.width = "50%"}
set.seed(4300)
# Create a vector of the values of x_99 and x_100
x = data3A$x[99:100]

# x_101 = beta_1 * x_99 + beta_2 * x_100 + residual
x.101.LS = as.vector(bootstrapped.LS%*%x)
x.101.LA = as.vector(bootstrapped.LA%*%x)
for (i in 1:B){
  x.101.LS[i] = x.101.LS[i] + sample(e.bootstrapped.beta.LS[i,], size = 1)
  x.101.LA[i] = x.101.LA[i] + sample(e.bootstrapped.beta.LA[i,], size = 1)
}

df = data.frame(x = x.101.LS)
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of x_101 based on LS estimators") + xlab("x_101")
df = data.frame(x = x.101.LA)
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of x_101 based on LA estimators") + xlab("x_101")
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```
```{r, echo = F, eval = T, out.width = "50%"}
set.seed(4300)
# Create a vector of the values of x_99 and x_100
x = data3A$x[99:100]

# x_101 = beta_1 * x_99 + beta_2 * x_100 + residual
x.101.LS = as.vector(bootstrapped.LS%*%x)
x.101.LA = as.vector(bootstrapped.LA%*%x)
for (i in 1:B){
  x.101.LS[i] = x.101.LS[i] + sample(e.bootstrapped.beta.LS[i,], size = 1)
  x.101.LA[i] = x.101.LA[i] + sample(e.bootstrapped.beta.LA[i,], size = 1)
}

df = data.frame(x = x.101.LS)
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of x_101 based on LS estimators") + xlab("x_101")
df = data.frame(x = x.101.LA)
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of x_101 based on LA estimators") + xlab("x_101")
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```
We observe that the histograms from the two different estimation methods look very similar. However, the limits of the $95\%$ prediction interval for the LA estimator are somewhat higher than the interval for the LS estimator. Still, this result could change if we set a different seed before sampling the residuals or before the bootstrap sampling.

If we study the histograms, we also see that we get some really large simulated values for $x_{101}$. However, these large values do not appear frequent enough to appear in the $95\%$ prediction intervals. The reason why these large values appear becomes evident if we plot a histogram of the residual distribution. This is done for the distribution based on LS estimates in figure \ref{fig:resdist}, and the distribution based on LA estimates will be very similar.

```{r, echo = T, eval = T, out.width = "50%", fig.align = "center", fig.cap="Histogram of residual distribution based on bootstrapped LS estimates of beta. \\label{fig:resdist}"}
df = data.frame(x= as.vector(e.bootstrapped.beta.LS))
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of residual distribution based on bootstrapped LS estimates of beta") + xlab("residual")
```
From figure \ref{fig:resdist}, we see that some residuals have a value of $\simeq 30$, while the rest are concentrated in the range $[-10,10]$. This explains the observed large values for $x_{101}$.

A different method to estimate the distribution of $x_{101}$ is to use the bootstrapped values of $\boldsymbol \beta$ and simulate the residuals from the initial observed residuals achieved wihtout bootstrapping. This is done below.


```{r, echo = T, eval = F}
set.seed(4300)
residuals = sample(e.observed.LS, size = B, replace = T)
x.101.LS = as.vector(bootstrapped.LS%*%x) + residuals
x.101.LA = as.vector(bootstrapped.LA%*%x) + residuals
df = data.frame(x = x.101.LS)
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of x_101 based on LS estimators") + xlab("x_101")
df = data.frame(x = x.101.LA)
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of x_101 based on LA estimators") + xlab("x_101")
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```
```{r, echo = F, eval = T, out.width = "50%"}
set.seed(4300)
residuals = sample(e.observed.LS, size = B, replace = T)
x.101.LS = as.vector(bootstrapped.LS%*%x) + residuals
x.101.LA = as.vector(bootstrapped.LA%*%x) + residuals
df = data.frame(x = x.101.LS)
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of x_101 based on LS estimators") + xlab("x_101")
df = data.frame(x = x.101.LA)
ggplot(df, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of x_101 based on LA estimators") + xlab("x_101")
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```
We see that both the prediction interval values, and the histograms achieved from this method...
Histogram ved å bruke e.observed
Histogrammene er veldig like, noe vi også forventer siden residuals burde bli ganske like i bootstrap regression of de faktiske


# Problem B: Permutation test

## 1)

We first inspect the logarithms of the concentrations for each individual.
```{r, echo=T, eval=T, out.width="50%", fig.align="center", fig.cap="Box plot of the logarithm of the measured concentration (mg/dL) in blood samples taken from three young men. \\label{boxplot}"}
bilirubin <- read.table("bilirubin.txt",header=T)
bilirubin['log_meas'] = log(bilirubin$meas)
ggplot(data = bilirubin, aes(x = pers, y = log_meas)) + geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE)
```
We observe from \ref{boxplot} that the median (the middle bold line) for person 1 and person 2 are very close to eachother and far from the median for person 3. From the 1st and 3rd quartiles of the boxplots (indicated by the lower and upper hinges) we observe that there is little spread in the measurements for person 2, some spread for person 3 and a lot of spread for person 1. The whiskers denote the largest / smallest measurement that is whithin $1.5 * IQR$ (the range between the 1st and 3rd quantile) from the upper / lower hinge. This measures the total spread in the data. For person 1 and person 2 we see that the whiskers extend relatively far into the range of the boxplot of person 3. Judging based only on the median we may be tempted to conclude already that the medians are significantly different from eachother, but observing (from the whiskers) the large spread in the data we should also investigate this significance quantitatively.

```{r}
model0 = lm(log(meas)~pers,data=bilirubin)
summary_model0 = summary(model0)
fstatistic_model0 = summary_model0$fstatistic[1]

summary_model0
```



## 2)

```{r}
set.seed(4300)
fstatistic_length = 999
fstatistic_values = c(length.out = fstatistic_length)

permTest <- function() {
  df = data.frame(bilirubin)
  df$pers = sample(bilirubin$pers, size=length(bilirubin$pers), replace = FALSE)
  sum = summary(model0)
  return(summary(lm(log(meas)~pers,data=df))$fstatistic['value'])
}

```

## 3)

```{r}
for (i in seq(0, fstatistic_length)) {
  fstatistic_values[i] = permTest()
}

df_fstatistic_values = data.frame(x = fstatistic_values)
fstatistic_quantiles = quantile(fstatistic_values, probs = c(0.05, 0.95))
ggplot() + geom_histogram(data = df_fstatistic_values, aes(x = x, y = ..density.., col="permuted"), bins=200) + geom_point(aes(x=fstatistic_model0, y=0, col="original")) + geom_vline(aes(xintercept = c(fstatistic_quantiles)))
```
```{r}
index = which.min(abs(sort(fstatistic_values) - fstatistic_model0))
cat("p-value: ", (fstatistic_length - index)/fstatistic_length)
```

