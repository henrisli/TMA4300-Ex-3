---
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 3, Spring 2019'
output:
  pdf_document:
    toc: no
    toc_depth: '2'
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Mikal Solberg Stapnes'
header-includes: \usepackage{float}
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
library(gridExtra)
library(ggpubr)
library(ggplot2)
library(coda)
library(cowplot)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE, fig.pos = 'htb')
```

# Problem A: Comparing `AR(2)` parameter estimators using resampling of residuals

In this exercise we want to analyse a non-Gaussian time-series, and compare two different parameter estimators. We consider and AR(2) model, which is specified by the relation
$$x_t = \beta_xx_{t-1}+\beta_2x_{t-2}+e_t,$$
where $e_t$ are i.i.d. random variables with zero mean and constant variance. The least sum of squared residuals (LS) and least sum of absolute residuals (LA) are obtained by minimising the following loss functions w.r.t $\boldsymbol \beta$

\begin{align*}
Q_{LS}(\mathbf{x}) &= \sum_{t=3}^T(x_t-\beta_1x_{t-1}-\beta_2x_{t-2})^2,\\
Q_{LA}(\mathbf{x}) &= \sum_{t=3}^T|x_t-\beta_1x_{t-1}-\beta_2x_{t-2}|.
\end{align*}

We denote the minimisers by $\hat{\boldsymbol \beta}_{LS}$ and $\hat{\boldsymbol \beta}_{LA}$, and define the estimated residuals to be $\hat{e}_t= x_t-\hat{\beta}_1x_{t-1}-\hat{\beta}_2x_{t-2}$ for $t=3,\dots,T$, and let $\bar{e}$ be the mean of these. Then we re-center the residuals to have mean zero by defining $\hat{\epsilon}_t = \hat{e}_t - \bar{e}$.
```{r, echo = F, eval = T}
source("probAdata.R")
source("probAhelp.R")
```
## 1.
We now use the residual resampling bootstrap method to evaluate the relative performance of the two parameter estimators. First, we calculate the two estimators by using the provided function `ARp.beta.est()`. Then, we calculate the observed residuals by using the provided function `ARp.resid()`.
```{r, echo = T, eval = T}
# Calculate the two estimators
n = length(data3A$x)
estimators = ARp.beta.est(data3A$x,2)
beta.LS = estimators$LS
beta.LA = estimators$LA

# Calculate observed residuals
e.observed.LS = ARp.resid(data3A$x,beta.LS)
e.observed.LA = ARp.resid(data3A$x,beta.LA)
```

Then, we want to estimate the variance and bias of the two estimators. We use 1500 bootstrap samples, each as long as the original data sequence. To do resampling, we initialise values for $x_1$ and $x_2$ by picking a random consecutive sequence from the data. Then, we use the provided function `ARp.filter()` to generate a new sample based on the bootstrapped residuals. Finally, we regress the new time-series to obtain bootstrapped estimates of the different $\boldsymbol \beta$'s.
```{r, echo = T, eval = T}
set.seed(4300)
# Number of Bootstrap Samples
B = 1500

# Bootstrap the residuals B times
e.bootstrapped = matrix(sample(e.observed.LS, size=B*(n-2), replace=T), nrow = B, ncol = n-2)

# Create a random consecutive sequence to initialise
index = sample(99, B, replace=T)
index.mat = matrix(c(index,index+1),nrow = B,ncol = 2)
x0 = matrix(data3A$x[index.mat], nrow = B, ncol = 2)

# Prepare to allocate data
bootstrapped.LS = matrix(NA, nrow = B, ncol = 2)
bootstrapped.LA = matrix(NA, nrow = B, ncol = 2)
# Prepare to allocate data to create residuals from the B different pairs of beta
e.bootstrapped.beta.LS = matrix(NA, nrow = B, ncol = n-2)
e.bootstrapped.beta.LA = matrix(NA, nrow = B, ncol = n-2)

for (i in 1:B){
  # Create time-series
  bootstrapped.sample.LS = ARp.filter(x0[i,], beta.LS, e.bootstrapped[i,])
  bootstrapped.sample.LA = ARp.filter(x0[i,], beta.LA, e.bootstrapped[i,])
  
  # Regress on the time-series
  bootstrapped.LS[i,] = ARp.beta.est(bootstrapped.sample.LS, 2)$LS
  bootstrapped.LA[i,] = ARp.beta.est(bootstrapped.sample.LA, 2)$LA
  
  # Compute the corresponding residuals based on the created time-series and the computed betas
  e.bootstrapped.beta.LS[i,] = ARp.resid(bootstrapped.sample.LS,bootstrapped.LS[i,])
  e.bootstrapped.beta.LA[i,] = ARp.resid(bootstrapped.sample.LA,bootstrapped.LA[i,])
}
```

We use the bootstrap to obtain variance and bias of the two estimators. To estimate the bias, we use the plug-in principle, and define the estimate of the bias as
$$\text{bias}_{\hat{F}_0} = E_{\hat{F}_0} [s(\mathbf{x}^\ast)]-\hat{\boldsymbol \beta},$$
where $\hat{F}_0$ is the bootstrap sample distribution, $\mathbf{x}^\ast$ is a bootstrap sample and $s(\cdot)$ is the bootstrap estimator. To estimate the variance, we use the sample variance of the bootstrap estimators.

```{r, echo = T, eval = F}
# Mean of bootstrap estimators
bootstrap.estimate.LS = apply(bootstrapped.LS,2,mean)
bootstrap.estimate.LA = apply(bootstrapped.LA,2,mean)

# Variance of bootstrap estimators
bootstrap.variance.LS = apply(bootstrapped.LS,2,var)
bootstrap.variance.LA = apply(bootstrapped.LA,2,var)
cat("Bias of beta.LS: ", bootstrap.estimate.LS - beta.LS, "\n")
cat("Bias of beta.LA: ", bootstrap.estimate.LA - beta.LA, "\n")

cat("Variance of beta.LS: ", bootstrap.variance.LS, "\n")
cat("Variance of beta.LA: ", bootstrap.variance.LA, "\n")
```
```{r, echo = F, eval = T}
# Mean of bootstrap estimators
bootstrap.estimate.LS = apply(bootstrapped.LS,2,mean)
bootstrap.estimate.LA = apply(bootstrapped.LA,2,mean)

# Variance of bootstrap estimators
bootstrap.variance.LS = apply(bootstrapped.LS,2,var)
bootstrap.variance.LA = apply(bootstrapped.LA,2,var)
cat("Bias of beta.LS: ", bootstrap.estimate.LS - beta.LS, "\n")
cat("Bias of beta.LA: ", bootstrap.estimate.LA - beta.LA, "\n")

cat("Variance of beta.LS: ", bootstrap.variance.LS, "\n")
cat("Variance of beta.LA: ", bootstrap.variance.LA, "\n")
```

We know that for a Gaussian $AR(p)$ process, the LS estimator will be optimal. However, from the print-out above, we see that the LA estimator both has smaller bias and smaller variance than the LS estimator. Our data is not Gaussian, which explains why the LA estimator is optimal and not the LS estimator.


## 2.
We now want to compute a $95\%$ prediction interval for $x_{101}$ based on both the LS and the LA estimator. We use the bootstrapped time-series and the 1500 estimates for $\boldsymbol \beta$ obtained in part 1. to estimate the residual distribution, then we use the following formula to simulate a value $x_{101}$ for the observed time-series
$$x_{101} = \beta_1 x_{99} + \beta_2 x_{100} + e_{101},$$
where $e_{101}$ is drawn from the residual distribution. By doing this, the simulated $x_{101}$ values will reflect both our lack of knowledge about the parameter values, and our lack of knowledge about the residual distribution. We then display histograms of the simulated distributions of $x_{101}$, and also find limits in the prediction interval as quantiles in the simulated values.

```{r, echo = T, eval = F, out.width = "60%"}
set.seed(4300)
# Create a vector of the values of x_99 and x_100
x = data3A$x[99:100]

# x_101 = beta_1 * x_99 + beta_2 * x_100 + residual
x.101.LS = as.vector(bootstrapped.LS%*%x)
x.101.LA = as.vector(bootstrapped.LA%*%x)
for (i in 1:B){
  x.101.LS[i] = x.101.LS[i] + sample(e.bootstrapped.beta.LS[i,], size = 1)
  x.101.LA[i] = x.101.LA[i] + sample(e.bootstrapped.beta.LA[i,], size = 1)
}

df.LS = data.frame(x = x.101.LS)
p1 <- ggplot(df.LS, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LS estimators") + xlab("x_101") + theme_grey(base_size = 8)
df.LA = data.frame(x = x.101.LA)
p2 <- ggplot(df.LA, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LA estimators") + xlab("x_101") + theme_grey(base_size = 8)
grid.arrange(p1,p2, nrow = 1)
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```
```{r, echo = F, eval = T, out.width='60%', fig.cap="\\label{fig:origest}Histogram of $x_{101}$ based on LS and LA estimates of $\\boldsymbol{\\beta}$ and residuals based on each bootstrapped time-series.",fig.align = "center"}
set.seed(4300)
# Create a vector of the values of x_99 and x_100
x = data3A$x[99:100]

# x_101 = beta_1 * x_99 + beta_2 * x_100 + residual
x.101.LS = as.vector(bootstrapped.LS%*%x)
x.101.LA = as.vector(bootstrapped.LA%*%x)
for (i in 1:B){
  x.101.LS[i] = x.101.LS[i] + sample(e.bootstrapped.beta.LS[i,], size = 1)
  x.101.LA[i] = x.101.LA[i] + sample(e.bootstrapped.beta.LA[i,], size = 1)
}

df.LS = data.frame(x = x.101.LS)
p1 <- ggplot(df.LS, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LS estimators") + xlab("x_101") + theme_grey(base_size = 8)
df.LA = data.frame(x = x.101.LA)
p2 <- ggplot(df.LA, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LA estimators") + xlab("x_101") + theme_grey(base_size = 8)
grid.arrange(p1,p2, nrow = 1)
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```

LEGGE TIL LINJER FOR 95% PRED INT?

We observe that the histograms from the two different estimation methods look very similar. However, the limits of the $95\%$ prediction interval for the LA estimator are somewhat higher than the interval for the LS estimator. Still, this result could change if we set a different seed before sampling the residuals or before the bootstrap sampling.

If we study the histograms, we also see that we get some really large simulated values for $x_{101}$. However, these large values do not appear frequent enough to appear in the $95\%$ prediction intervals. The reason why these large values appear becomes evident if we plot a histogram of the residual distribution. This is done for the distribution based on LS estimates in figure \ref{fig:resdist}, and the distribution based on LA estimates will be very similar.

```{r, echo = T, eval = T, out.width = "60%", fig.align = "center", fig.pos="H", fig.cap="Histogram of residual distribution based on bootstrapped LS estimates of $\\boldsymbol{\\beta}$ and original LS estimate of $\\boldsymbol{\\beta}$. \\label{fig:resdist}"}
df.e = data.frame(x= as.vector(e.bootstrapped.beta.LS))
df.e.b = data.frame(x = e.observed.LS)
p3 <- ggplot(df.e, aes(x = x)) + geom_histogram(aes(y = ..density..), bins = 100) + ggtitle("Histogram of residual distribution based \n on bootstrapped LS estimates of beta") + xlab("residual") + theme_grey(base_size = 8)
p4 <- ggplot(df.e.b, aes(x = x)) + geom_histogram(aes(y = ..density..), bins = 100) + ggtitle("Histogram of residual distribution based \n on original LS estimate of beta") + xlab("residual") + theme_grey(base_size = 8)
grid.arrange(p3,p4, nrow = 1)
```
From figure \ref{fig:resdist}, we see that some residuals have a value of $\simeq 30$, while the rest are concentrated in the range $[-10,10]$. This explains the observed large values for $x_{101}$.


We also observe that the residuals from the original LS estimate has almost the same distribution as the residuals from the bootstrapped LS estimates. Thus, we propose a new method to estimate the distribution of $x_{101}$: We use the bootstrapped values of $\boldsymbol \beta$ and simulate the residuals from the initial observed residuals achieved wihtout bootstrapping. This is done below.


```{r, echo = T, eval = F}
set.seed(4300)
residuals = sample(e.observed.LS, size = B, replace = T)
x.101.LS = as.vector(bootstrapped.LS%*%x) + residuals
x.101.LA = as.vector(bootstrapped.LA%*%x) + residuals
df.bs.LS = data.frame(x = x.101.LS)
p5 <- ggplot(df.bs.LS, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LS estimators") + xlab("x_101") + theme_grey(base_size = 8)
df.bs.LA = data.frame(x = x.101.LA)
p6 <- ggplot(df.bs.LA, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LA estimators") + xlab("x_101") + theme_grey(base_size = 8)
grid.arrange(p5,p6, nrow = 1)
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```
```{r, echo = F, eval = T, out.width='60%', fig.pos="H", fig.cap="\\label{fig:origest}Histogram of $x_{101}$ based on LS and LA estimates of $\\boldsymbol{\\beta}$ and residuals based on the original time-series.",fig.align = "center"}
set.seed(4300)
residuals = sample(e.observed.LS, size = B, replace = T)
x.101.LS = as.vector(bootstrapped.LS%*%x) + residuals
x.101.LA = as.vector(bootstrapped.LA%*%x) + residuals
df.bs.LS = data.frame(x = x.101.LS)
p5 <- ggplot(df.bs.LS, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LS estimators") + xlab("x_101") + theme_grey(base_size = 8)
df.bs.LA = data.frame(x = x.101.LA)
p6 <- ggplot(df.bs.LA, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LA estimators") + xlab("x_101") + theme_grey(base_size = 8)
grid.arrange(p5,p6, nrow = 1)
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```
We see that both the prediction interval values, and the histograms achieved from this method are very similar to the first method. This result is as expected, as the residuals should have the same distribution due to the nature of bootstrapping. The bootstrapped time-series are created by bootstrapping the observed residuals, and the time-series are then used to create the residuals used in the first method.

# Problem B: Permutation test

## 1)

We wish to inspect the concentrations of bilirubin in the blood of three young men for possible evidence of health issues. We first inspect the logarithms of the measured concentrations for each individual.
```{r, echo=T, eval=T, out.width="50%", fig.pos="H", fig.align="center", fig.cap="Box plot of the logarithm of the measured concentration (mg/dL) in blood samples taken from three young men. \\label{boxplot}"}
bilirubin <- read.table("bilirubin.txt",header=T)
bilirubin['log_meas'] = log(bilirubin$meas)
ggplot(data = bilirubin, aes(x = pers, y = log_meas)) + geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE) + theme_grey()
```
We observe from figure \ref{boxplot} that the median (the middle bold line) for person 1 and person 2 are very close to eachother and far from the median for person 3. From the 1st and 3rd quartiles of the boxplots (indicated by the lower and upper hinges) we observe that there is little spread in the measurements for person 2, some spread for person 3 and a lot of spread for person 1. The whiskers denote the largest / smallest measurement that is whithin $1.5 * IQR$ (the range between the 1st and 3rd quantile) from the upper / lower hinge. This measures the total spread in the data. For person 1 and person 2 we see that the whiskers extend relatively far into the range of the boxplot of person 3. The medians indicate a large difference between the bilirubin levels of person 1 / 2 and person 3 but observing (from the whiskers) the large spread in the data we should also investigate this significance quantitatively.

We fit a linear model $$\log Y_{ij} \sim \beta_{p_j} + e_{ij}, \quad e_{ij} \overset{iid}{\sim} N(0, \sigma^2)$$

```{r}
model0 = lm(log(meas)~pers,data=bilirubin)
summary_model0 = summary(model0)
fstatistic_model0 = summary_model0$fstatistic[1]

summary_model0
```
From the summary we can read that, as expected, the coefficient for `persp3` is large. We can also read that the hypothesis 

$$ H_0: \beta_{p_1} = \beta_{p_2} = \beta_{p_3} \quad \text{vs} \quad  H_1: \beta_{p_1} \neq \beta_{p_2} \lor \beta_{p_1} \neq \beta_{p_3} \lor \beta_{p_2} \neq \beta_{p_3}$$
attains the F-value `r round(as.numeric(fstatistic_model0), 4)` with a corresponding p-value of `r round(1 - pf(fstatistic_model0, summary_model0$fstatistic[2], summary_model0$fstatistic[3]), 4)`, which is below any reasonable significance level. We note that this F-statistic is an estimator $\theta(x)$ and denote this original value of the estimator  $\hat{F}_0$. 

Based on the p-value corresponding to $\hat{F}_0$ we may be tempted to already conclude that the total regression is signficant and thus that person 3 is accumulating bilirubin in his blood. However, we now wish to investigate the distribution of $\hat{F}$ under $H_0$.

## 2)

We wish to investigate how, under $H_0$, likely it is to observe something equally or more extreme than $\hat{F}_0$. Assuming first $H_0$ to be true, we can argue that observing a permutation of the observed measurements would be equally likely as observing our original observations. We may therefore shuffle the measurements between the different men and compute a new F_statistic $\hat{F}^{(1)}_{H_0}$. We reshuffle $999$ times and compute $\{\hat{F}^{(1)}_{H_0}, \hat{F}^{(2)}_{H_0}, \dots, \hat{F}^{(999)}_{H_0}\}$.

```{r}
set.seed(4300)
fstatistic_length = 999
fstatistic_values = c(length.out = fstatistic_length)

permTest <- function() {
  dataframe = data.frame(bilirubin)
  dataframe$pers = sample(bilirubin$pers, size=length(bilirubin$pers), replace = FALSE)
  sum = summary(model0)
  return(summary(lm(log(meas)~pers,data=dataframe))$fstatistic['value'])
}
```


## 3)
We may use these $999$ samples as an estimate of the distribution of $\hat{F}$ under $H_0$. 

```{r, echo=T, eval=T, out.width="50%", fig.align="center", fig.pos="H", fig.cap="Histogram of the samples $\\{\\hat{F}^{(i)}_{H_0}\\}$. The lines indicate the $95\\%$ quantile, the value of the original estimate $\\hat{F}_0$ and the Fisher-distribution corresponding to the degrees of freedom in the regression. \\label{fig:histogram}"}

# Permute observations
for (i in seq(0, fstatistic_length)) {
  fstatistic_values[i] = permTest()
}


df_fstatistic_values = data.frame(x = fstatistic_values)
fstatistic_quantiles = quantile(fstatistic_values, probs = c(0.95))
ggplot() + geom_histogram(data = df_fstatistic_values, aes(x = x, y = ..density.., col="permuted"), bins=100) + stat_function(data = data.frame(x = c(0, 6)), lwd=1., aes(x, col=paste("F", summary_model0$fstatistic[2],"," ,summary_model0$fstatistic[3],"-distribution", sep="")), fun = df, args = list(df1 = summary_model0$fstatistic[2], df2 = summary_model0$fstatistic[3])) + geom_vline(aes(xintercept = c(fstatistic_quantiles), col="95% quantile"), lwd=1.) + geom_vline(aes(xintercept = fstatistic_model0, col="original"), lwd=1.) + xlim(0, 6) + theme_grey()
```


We observe from figure \ref{fig:histogram} that under $H_0$ the majority of the estimates fall below that of the original value of $\hat{F}_0$, but we also observe some larger estimates. From the $95\%$ quantile, we note that $\hat{F}_0$ is larger than at least $95\%$ of the estimates $\{\hat{F}^{(i)}_{H_0}\}$. To be more exact we can estimate the p-value of $\hat{F}$ under $H_0$, i.e. the probability of observing a value equally or more extreme than $\hat{F}_0$ by counting the number of observations $\{\hat{F}^{(i)}_{H_0}\}$ that are larger then $\hat{F}_0$. 

```{r}
index = which.min(abs(sort(fstatistic_values) - fstatistic_model0))
cat("p-value: ", (fstatistic_length - index)/fstatistic_length)
```

Here we observe an estimate of the p-value to be `r (fstatistic_length - index)/fstatistic_length`, which corresponds well with the original p-value of `r 1 - pf(fstatistic_model0, summary_model0$fstatistic[2], summary_model0$fstatistic[3])`. We could have foreseen that these p-values would be similar by observing, from figure \ref{fig:histogram}, that the distribution of $\{\hat{F}^{(i)}_{H_0}\}$ is very similar to the corresponding Fisher distribution. This gives us reason to believe that the assumptions of the F-test, i.e. that the regression errors are iid. normal, are satisfied and that F-test will yield a correct p-value. 

Regardless of our choice of p-value we can argue that the total regression is significant and reject $H_0$ on a $0.05$ significance level. This gives us reason to believe that person 3 is accumulating bilirubin in his blood.

# Problem C: The EM-algorithm and bootstrapping

We let $\{x_i\}_{i=1,\dots,n}$ be i.i.d. $\text{Exp}(\lambda_0)$ and $\{y_i\}_{i=1,\dots,n}$ be i.i.d. $\text{Exp}(\lambda_1)$. We assume that we observe neither $x_i$ nor $y_i$ directly, but rather observe

$$z_i = \max(x_i, y_i), \quad \text{and} \quad u_i = I(x_i \geq y_i), \quad i = 1, \dots, n,$$
where $I(\cdot)$ is the indicator function. This means that we only observe the largest of the pair $(x_i, y_i)$ and we know whether the observed value is $x_i$ or $y_i$.

Based on the observed $(z_i,u_i) \quad i= 1,\dots,n$ we want use the Expecation-Maximization algorithm to find the maximum likelihood estimates for $(\lambda_0,\lambda_1)$.

## 1.

We first formulate the conditional probability of the complete data $\mathbf{x} = (x_1,\dots,x_n)$ and $\mathbf{y} = (y_1,\dots,y_n)$ given $(\lambda_0, \lambda_1)$. 

$$ f_{X_i, Y_i} (x_i, y_i \lvert \lambda_0, \lambda_1) = f_{X_i} (x_i \lvert \lambda_0) f_{Y_i} (y_i \lvert \lambda_1)= \lambda_0 e^{-\lambda_0 x_i } \lambda_1 e^{-\lambda_1 y_i },$$
because $x_i$ and $y_i$ are assumed independent. This gives
$$f(\mathbf{x}, \mathbf{y} \lvert \lambda_0, \lambda_1) = \prod_{i=1}^n f_{X_i, Y_i} (x_i, y_i \lvert \lambda_0, \lambda_1) = (\lambda_0 \lambda_1)^n \exp \big (-\lambda_0 \sum_{i=1}^n x_i \big )\exp \big (-\lambda_1 \sum_{i=1}^n y_i \big ),$$
and finally the loglikelihood
$$ \ln f(\mathbf{x}, \mathbf{y} \lvert \lambda_0, \lambda_1) = n (\ln \lambda_0 + \ln \lambda_1) -\lambda_0 \sum_{i=1}^n x_i   -\lambda_1 \sum_{i=1}^n y_i.$$

We now find the conditional probabilities of $x_i$ and $y_i$ given $z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}$

$$f_{X_i} (x_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}) = \begin{cases}\frac{\lambda_0^{(t)} \exp \{ -\lambda_0^{(t)} x_i \}}{1 - \exp \{-\lambda_0^{(t)} z_i \}}, \quad &u_i = 0, \\
z_i, \quad &u_i = 1,
\end{cases}$$

$$f_{Y_i} (y_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}) = \begin{cases}z_i, \quad &u_i = 0, \\
\frac{\lambda_1^{(t)} \exp \{ -\lambda_1^{(t)} y_i \}}{1 - \exp \{-\lambda_1^{(t)} z_i \}}, \quad &u_i = 1.
\end{cases}$$

The EM algorithm is based on maximizing the expectation of the loglikelihood of $\ln f(\mathbf{x}, \mathbf{y} \lvert \lambda_0, \lambda_1)$ given $\mathbf{z}, \mathbf{u}$ and some initial guess of the intensities $\lambda_0^{t}$ and $\lambda_1^{t}$, 

$$\begin{aligned} E_{\mathbf{x}, \mathbf{y}} [ \ln f(\mathbf{x}, \mathbf{y} \lvert \lambda_0, \lambda_1) \lvert \mathbf{z}, \mathbf{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] &= n (\ln \lambda_0 + \ln \lambda_1)  \\
& -\lambda_0 \sum_{i=1}^nE[x_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)} ] \\
& -\lambda_1 \sum_{i=1}^n E[y_i, \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}],
\end{aligned}
$$

with respect to $(\lambda_0, \lambda_1)$. The expectations $E[x_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}]$ and $E[y_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}]$ can be computed as

$$ E[x_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}] = u_i z_i + (1-u_i) \int_0^{z_i} x_i \frac{\lambda_0^{(t)} \exp \{ -\lambda_0^{(t)} x_i \}}{1 - \exp \{-\lambda_0^{(t)} z_i \}} dx_i 
\\ = u_i z_i + (1 - u_i) \bigg (\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\{\lambda_0^{(t)} z_i\} - 1} \bigg),
$$
and
$$ E[y_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}] = (1-u_i) z_i + u_i \int_0^{z_i} y_i \frac{\lambda_1^{(t)} \exp \{ -\lambda_1^{(t)} y_i \}}{1 - \exp \{-\lambda_1^{(t)} z_i \}} dy_i 
\\ = (1-u_i) z_i + u_i \bigg (\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\{\lambda_1^{(t)} z_i\} - 1} \bigg).$$

To maximize this expectation, we define the function $Q(\cdot)$, 

$$\begin{aligned} 
Q(\lambda_0, \lambda_1 \lvert \lambda_0^{(t)}, \lambda_1^{(t)}) = E_{\mathbf{x}, \mathbf{y}} [ \ln f(\mathbf{x}, \mathbf{y} \lvert \lambda_0, \lambda_1) \lvert \mathbf{z}, \mathbf{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] 
&=  n (\ln \lambda_0 + \ln \lambda_1) \\ 
&-\lambda_0 \sum_{i=1}^n u_i z_i + (1 - u_i) \bigg (\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp\{\lambda_0^{(t)} z_i\} - 1} \bigg) \\
&- \lambda_1 \sum_{i=1}^n  (1-u_i) z_i + u_i \bigg (\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\{\lambda_1^{(t)} z_i\} - 1} \bigg).
\end{aligned}$$

## 2.

This function $Q(\lambda_0, \lambda_1 \lvert \lambda_0^{(t)}, \lambda_1^{(t)})$ admits a maximum, which we can find using

$$\frac{\partial }{\partial \lambda_0} Q= 0 \quad\quad \frac{\partial }{\partial \lambda_1}Q= 0$$

Giving 
\begin{align*} \lambda_0 &= \frac{n}{\sum_{i=1}^n u_i z_i + (1 - u_i) \bigg (\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp{\lambda_0^{(t)} z_i} - 1} \bigg)},
\\ \lambda_1 &= \frac{n}{\sum_{i=1}^n  (1-u_i) z_i + u_i \bigg (\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp\{\lambda_1^{(t)} z_i\} - 1} \bigg)},
\end{align*}

which gives us the recursion $(\lambda_0^{t}, \lambda_1^{t}) \mapsto (\lambda_0^{(t+1)}, \lambda_1^{(t+1)})$. We implement this,

```{r, echo=T, eval=T, out.width="50%", fig.align="center", fig.pos="H", fig.cap="Convergence plot for $(\\lambda_0, \\lambda_1)$ for finding the maximum likelihood estimates using recursion. \\label{fig:conv_lamb}"}
u = read.table("https://www.math.ntnu.no/emner/TMA4300/2019v/exercise3/ex3-additionalFiles/u.txt")[,1]
z = read.table("https://www.math.ntnu.no/emner/TMA4300/2019v/exercise3/ex3-additionalFiles/z.txt")[,1]
n = length(u)

n.iterations = 30
lambda = array(dim=c(n.iterations, 2))
lambda[1,] = c(1, 15)


for(i in 2:n.iterations){
  lambda[i, 1] = n / sum(u*z + (1-u)*(1/lambda[i-1, 1] - z / (exp(lambda[i-1, 1]*z) - 1)))
  lambda[i, 2] = n / sum((1-u)*z + u*(1/lambda[i-1, 2] - z / (exp(lambda[i-1, 2]*z) - 1)))
}

lambda_orig = lambda[n.iterations,]

df = data.frame(n = 1:n.iterations, lambda)

ggplot() + geom_line(data = df, aes(x = n, y = X1, col="lambda_0")) + geom_line(data = df, aes(x = n, y = X2, col="lambda_1")) + ylab("") + ggtitle("Convergence plot for lambda_0 and lambda_1") + theme_grey()
```

and see from \ref{fig:conv_lamb} that the algorithm converges to the values $\lambda_{0}^{(0)}, \lambda_{1}^{(0)}$ = `r lambda_orig` after only 2-4 iterations. 

## 3).

This EM-algorithm is an estimator of $(\lambda_0, \lambda_1)$. We now want to investigate the bias and variance of this estimator. To do this we implement a bootstrap algorithm. The algorithm is summarized in the following pseudocode. 

for $b$ in $(1, 2, \dots, B)$ do
\begin{enumerate}
  \item $u_{b}^*$ = sample with replacement from $u$
  \item $z_{b}^*$ = sample with replacement from $z$
  \item $(\lambda_{0, b}^{*}, \lambda_{1, b}^{*})$ = run EM algorithm with $u_{b}^*$ and $z_{b}^*$
  \item store $(\lambda_{0, b}^{*}, \lambda_{1, b}^{*})$ as a single bootstrap estimate.
\end{enumerate}

We implement this in the following code

```{r, echo=T, eval=T, out.width="80%", fig.align="center", fig.pos="H", fig.cap="The distribution of the bootstrap samples of the EM-estimates of $(\\lambda_0, \\lambda_1)$. \\label{fig:bs_lamb}"}
algorithmEM = function(u,z){
  lambda = 5*runif(2)
  lambda_old = c(-1, -1)
  while(sum((lambda - lambda_old)^2)>2.2e-16){
    lambda_old = lambda
    lambda[1] = n / sum(u*z + (1-u)*(1/lambda[1] - z / (exp(lambda[1]*z) - 1)))
    lambda[2] = n / sum((1-u)*z + u*(1/lambda[2] - z / (exp(lambda[2]*z) - 1)))
  }
  return(lambda)
}

n_bs = 20000
lambda_bs = array(dim = c(n_bs, 2))

for (i in 1:n_bs){
  index = sample(1:n, replace=T)
  u_bs = u[index]
  z_bs = z[index]
  lambda_bs[i,] = algorithmEM(u_bs, z_bs)
}

dfbs1 = data.frame(idx = "lambda_0", x = lambda_bs[, 1], x0 = lambda_orig[1], mean = mean(lambda_bs[, 1]))
dfbs2 = data.frame(idx = "lambda_1", x = lambda_bs[, 2], x0 = lambda_orig[2], mean = mean(lambda_bs[, 2]))
df_bs = rbind(dfbs1, dfbs2)

ggplot() + geom_histogram(data = df_bs, aes(x = x, y = ..density.., col="bootstrap samples"), bins=100) + geom_vline(aes(xintercept = x0, col="original EM"), data = df_bs, lwd=1.0) +
geom_vline(aes(xintercept = mean, col="bootstrap mean"), data = df_bs, lwd=1.0) + xlim(0,16) + facet_wrap(~idx) + theme_grey()
```

From the lines in figure \ref{fig:bs_lamb} and the corresponding printout below we see that the mean $(\bar{\lambda}_0^*, \bar{\lambda}_1^*)$ of the bootstrap samples correspond well with the original EM estimate $\lambda_{0}^{(0)}, \lambda_{1}^{(0)}$.

```{r, echo=T, eval=T}
rbind(lambda_orig, lambda_bs_mean = apply(lambda_bs,2,mean))
```

However, from the width of the distribution of the bootstrap samples in figure \ref{fig:bs_lamb} we observe significant variance in the bootstrap samples, especially for $\lambda_1$. Comparing the bias estimate, $\widehat{\text{bias}} = (\bar{\lambda}_0^* -  \lambda_{0}^{(0)},  \bar{\lambda}_1^* - \lambda_{1}^{(0)})$, and standard deviations of the estimates

```{r, echo=T, eval=T}
rbind(bias = apply(lambda_bs,2,mean) - lambda_orig, stdev =  sqrt(diag(cov(lambda_bs))))
```

we see that there is non-negligible bias. We could consider using instead the bias-corrected EM estimator. However, we also see that the standard deviation of the estimates are about a magnitude larger than the bias. As the variance in this case is the main contributor to the error of the estimator we prefer the original EM estimate instead of the bias-corrected estimate. The bias-corrected estimator will give larger variance and thus less accurate estimates of the intensities $\lambda_0$ and $\lambda_1$. 

We observe that the correlation between the bootstrap estimates of $lambda_0$ and $lambda_1$, 

```{r, echo=T, eval=T}
cat("Corr", cor(lambda_bs)[1, 2], " ")
```

is very weak. This is as expected as our original variables $x_i$ and $y_i$ are sampled independently. 

## 4).

As an alternative to this EM approach we can formulate the Maximum Likelhood Estimator (MLE) of $(\lambda_0, \lambda_1)$ and compute it using classical optimization techniques. We first calculate the probablity that $Z_i \leq z$ and $u_i = 1$. We can find this by integrating the joint pdf of $(x_i, y_i)$

$$f_{X_i, Y_i}(x_i, y_i) = \lambda_0 e^{-\lambda_0 x_i} \lambda_1 e^{-\lambda_1 y_i}$$

over the domain $x_i \in [0, z]$, $y_i \in [0, x_i]$. This becomes

\begin{align*} p(Z_i \leq z, U_i = 1)  &= \int_0^{z} \int_0^{x_i} \lambda_0 e^{-\lambda_0 x_i} \lambda_1 e^{-\lambda_1 x_i} dy_i dx_i \\
 &= \int_0^z \lambda_0 e^{-\lambda_0 x_i} (1-e^{-\lambda_1 x_i})dx_i. \\
 \end{align*}


Using the fundamental theorem of calculus we get the pdf of $(z_i, u_i)$,

$$p(Z_i = z, U_i = 1) = \frac{d P(Z_i \leq z \lvert U_i = 1)}{dz} = \lambda_0 e^{-\lambda_0 z_i}(1-e^{-\lambda_1 z_i}).$$

Using the same technique for $u_i = 0$ we get

$$p(Z_i = z, U_i = 0) = \lambda_1 e^{-\lambda_1 z_i}(1-e^{-\lambda_0 z_i})$$

We combine this to formulate the joint density of $z_i, u_i$ given $\lambda_0, \lambda_1$,

$$f_{Z_i, U_i}(z_i, u_i ; \lambda_0, \lambda_1) =\begin{cases}
\lambda_1 e^{-\lambda_1 z_i}(1-e^{-\lambda_0 z_i}), &\text{ if }u_i=0, \\
\lambda_0 e^{-\lambda_0 z_i}(1-e^{-\lambda_1 z_i}), &\text{ if }u_i=1, \end{cases}
$$

which we use to compute the loglikelihood $\ell(\lambda_0, \lambda_1 ; \mathbf{z}, \mathbf{u})$

\begin{equation}
\begin{split}
\ell (\lambda_0, \lambda_1 ; \mathbf{z}, \mathbf{u}) = \log\big(f_{\mathbf{Z},\mathbf{U}}(\mathbf{z},\mathbf{u} ; \lambda_0, \lambda_1)\big) &= \log\Big(\prod_{i=1}^n f_{Z_i, U_i}(z_i, u_i)\Big) = \sum_{i=1}^n \log \big(f_{Z_i, U_i}(z_i, u_i ; \lambda_0, \lambda_1) \big)\\
&= \underset{u_i=0}{\sum_{i=1}^n}\log(\lambda_1)-z_i(\lambda_0+\lambda_1)+\log(e^{\lambda_0z_i}-1)\\
&+ \underset{u_i=1}{\sum_{i=1}^n}\log(\lambda_0)-z_i(\lambda_0+\lambda_1)+\log(e^{\lambda_1z_i}-1).
\end{split}
\end{equation}

Before continuing we note that if $z_i > 0$ for some $i$, which is very reasonable, the negative of the loglikelihood is coercive and the problem permits an optimal solution. To find this we differentiate the loglikelihood w.r.t. $\lambda_0$ and $\lambda_1$,

$$ \frac{\partial \ell (\lambda_0, \lambda_1 ; \mathbf{z}, \mathbf{u})}{\partial \lambda_0} = \frac{N_1}{\lambda_0} - \sum_{i=1}^n z_i + \underset{u_i=0}{\sum_{i=1}^n}\frac{z_ie^{z_i\lambda_0}}{e^{z_i\lambda_0}-1},$$
$$ \frac{\partial \ell (\lambda_0, \lambda_1 ; \mathbf{z}, \mathbf{u})}{\partial \lambda_1} = \frac{N_0}{\lambda_1} - \sum_{i=1}^n z_i + \underset{u_i=1}{\sum_{i=1}^n}\frac{z_ie^{z_i\lambda_1}}{e^{z_i\lambda_1}-1},$$
with $N_0=\sum_{i=1}^nI(u_i=0)$ and $N_1=\sum_{i=1}^nI(u_i=1)$.

We observe that the hessian,

$$ \nabla^2 \ell (\lambda_0, \lambda_1 ; \mathbf{z}, \mathbf{u}) = 
\begin{bmatrix} -\frac{N_1}{\lambda_0^2} - \underset{u_i=0}{\sum_{i=1}^n}\frac{z_i^2e^{z_i\lambda_0}}{(e^{z_i\lambda_0}-1)^2} & 0 \\
0 & -\frac{N_0}{\lambda_1^2} - \underset{u_i=0}{\sum_{i=1}^n}\frac{z_i^2e^{z_i\lambda_1}}{(e^{z_i\lambda_1}-1)^2}
\end{bmatrix}
$$
is negative definite for all $(\lambda_0, \lambda_1)$. We can combine this with the coercivity of the loglikelihood function and conclude that the optimal solution for $(\lambda_0, \lambda_1)$ exists and is uniquely solved by the 1st order optimality conditions 

$$ \frac{\partial \ell (\lambda_0, \lambda_1 ; \mathbf{z}, \mathbf{u})}{\partial \lambda_0}=0, \quad \text{and} \quad \frac{\partial \ell (\lambda_0, \lambda_1 ; \mathbf{z}, \mathbf{u})}{\partial \lambda_1} = 0.$$

However, we here choose to optimize the loglikelihood using the derivative-free optimization technique implemented in R's `optim` function. This is an implementation of the Nelder-Mead algorithm.

```{r, echo=T, eval=T, out.width="50%", fig.align="center", fig.pos="H", fig.cap="Contour-plot of the log-likelihood defined above as well as its optimum, the original EM-estimate $(\\lambda_0^{(0)}, \\lambda_1^{(0)})$ and the bootstrap mean $(\\bar{\\lambda}_0^*, \\bar{\\lambda}_1^*)$. \\label{fig:llik_lamb}"}

#Loglikelihood as defined above
loglik = function(x){
  lambda_0 = x[1]
  lambda_1 = x[2]
  
  res = sum(u*log(lambda_0 * (1 - exp(-lambda_1 * z)) * exp(-lambda_0 * z)))
  res = res + sum((1-u)*log(lambda_1 * (1 - exp(-lambda_0 * z)) * exp(-lambda_1 * z)))

  return(res)
}

loglik2 = function(x){
  lambda_0 = x[1]
  lambda_1 = x[2]
  return(sum(-z[which(u==0)]*(lambda_0+lambda_1)+log(exp(lambda_0*z[which(u==0)])-1)+log(lambda_1)) + sum(-z[which(u==1)]*(lambda_0+lambda_1)+log(exp(lambda_1*z[which(u==1)])-1)+log(lambda_0)))
}

# Find optimal lambda using derivative-free optimization with initial guess of (4, 12)
lambda_optim = optim(par=c(4, 12), fn=loglik, control = list(fnscale = -1, maxit=1000, reltol=1e-16, ndeps=1e-6))$par

# Define area as [1, 7] x [1, 15]
x0 = seq(2.5, 4.5, length.out = 30)
x1 = seq(6, 12, length.out = 30)
out = array(dim = c(30, 30, 3))

# Compute loglikelihood over area
for (i in 1:30){
  for (j in 1:30){
    out[i, j, 1] = x0[i]
    out[i, j, 2] = x1[j]
    out[i, j, 3] = loglik(c(x0[i], x1[j]))
  }
}

dfll = data.frame(lambda_0 = as.vector(out[,, 1]), lambda_1 = as.vector(out[,,2]), llik = as.vector(out[,,3]))
dfll_dot1 = data.frame(lambda_0 = lambda_orig[1], lambda_1 = lambda_orig[2])
dfll_dot3 = data.frame(lambda_0 = mean(lambda_bs[, 1]), lambda_1 = mean(lambda_bs[, 2]))
dfll_dot2 = data.frame(lambda_0 = lambda_optim[1], lambda_1 = lambda_optim[2])

ggplot() + geom_contour(data = dfll, aes(x = lambda_0, y = lambda_1, z = llik), binwidth=3.5, show.legend=T)  + geom_point(data = dfll_dot1, aes(x = lambda_0, y = lambda_1, shape="EM-estimate", col="EM-estimate"), size=4, alpha=0.7) + geom_point(data = dfll_dot2, aes(x = lambda_0, y = lambda_1, shape="optimized", col="optimized"), size=4, alpha=0.7) + geom_point(data = dfll_dot3, aes(x = lambda_0, y = lambda_1, shape="bootstrap mean", col="bootstrap mean"), size=4, alpha=0.7) + theme(legend.position = c(0.95, 0.95), legend.justification = c("right", "top"))
```

We see from the contour plot of the loglikelihood in figure \ref{fig:llik_lamb} that the optimization scheme correctly finds the ML estimate $(\hat{\lambda}_0, \hat{\lambda}_1)$ of $(\lambda_0, \lambda_1)$. From the overlapping dots and from the corresponding printout below we see that the ML estimate and the original EM estimate are very similar, while the bootstrap mean is somewhat different.

```{r, echo=T, eval=T}
rbind(lambda_orig, lambda_optim, lambda_bs_mean = apply(FUN=mean, lambda_bs, MARGIN=2))
```

In general, the linear convergence of EM can be slow compared to the convergence of classical optimization techniques. With the analytical likelihood, we can formulate the derivatives of the loglikelihood and rely on the well-developed theory of derivative-based optimization techniques. If we also formulate the hessian we can attain the quadratic convergence of the Newton's method. In practice we can then also use optimization frameworks such as R's `optim` function or Python's `scipy.linalg`.

Additionally, it may be difficult to formulate fitting convergence criteria for the EM algorithm. Typically the algorithm is set to terminate when the norm of the change from one iteration to the next reaches some lower value. If the convergence criteria of the EM algorithm is disconnected from the problem in question, it is possible that the algorithm will terminate before having reached the optimal solution. If the slope of the loglikelihood around the optimal solution is very steep, changes that are small in norm may cause large differences in the loglikelihood, and the lower value for the increment must be chosen accordingly. However, formulating good a priori estimates for the correct lower value is difficult. For MLE, the convergence criteria of derivative-based optimization algorithms are based on the norm of the gradient of the loglikelihood and therefore less prone to premature termination. 

However, the ease of implementation and the stable ascent of EM are often very attractive despite its slow convergence.
