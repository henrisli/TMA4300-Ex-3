---
title: 'TMA4300 Computer Intensive Statistical Methods Exercise 3, Spring 2019'
output:
  pdf_document:
    toc: no
    toc_depth: '2'
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group members: Henrik Syversveen Lie, Mikal Solberg Stapnes'
header-includes: \usepackage{float}
---


```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
library(gridExtra)
library(ggpubr)
library(ggplot2)
library(coda)
library(cowplot)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE, fig.pos = 'htb')
```

# Problem A: Comparing `AR(2)` parameter estimators using resampling of residuals

In this exercise we want to analyse a non-Gaussian time-series, and compare two different parameter estimators. We consider and AR(2) model, which is specified by the relation
$$x_t = \beta_xx_{t-1}+\beta_2x_{t-2}+e_t,$$
where $e_t$ are i.i.d. random variables with zero mean and constant variance. The least sum of squared residuals (LS) and least sum of absolute residuals (LA) are obtained by minimising the following loss functions w.r.t $\boldsymbol \beta$

\begin{align*}
Q_{LS}(\mathbf{x}) &= \sum_{t=3}^T(x_t-\beta_1x_{t-1}-\beta_2x_{t-2})^2,\\
Q_{LA}(\mathbf{x}) &= \sum_{t=3}^T|x_t-\beta_1x_{t-1}-\beta_2x_{t-2}|.
\end{align*}

We denote the minimisers by $\hat{\boldsymbol \beta}_{LS}$ and $\hat{\boldsymbol \beta}_{LA}$, and define the estimated residuals to be $\hat{e}_t= x_t-\hat{\beta}_1x_{t-1}-\hat{\beta}_2x_{t-2}$ for $t=3,\dots,T$, and let $\bar{e}$ be the mean of these. Then we re-center the residuals to have mean zero by defining $\hat{\epsilon}_t = \hat{e}_t - \bar{e}$.
```{r, echo = F, eval = T}
source("probAdata.R")
source("probAhelp.R")
```
## 1.
We now use the residual resampling bootstrap method to evaluate the relative performance of the two parameter estimators. First, we calculate the two estimators by using the provided function `ARp.beta.est()`. Then, we calculate the observed residuals by using the provided function `ARp.resid()`.
```{r, echo = T, eval = T}
# Calculate the two estimators
n = length(data3A$x)
estimators = ARp.beta.est(data3A$x,2)
beta.LS = estimators$LS
beta.LA = estimators$LA

# Calculate observed residuals
e.observed.LS = ARp.resid(data3A$x,beta.LS)
e.observed.LA = ARp.resid(data3A$x,beta.LA)
```

Then, we want to estimate the variance and bias of the two estimators. We use 1500 bootstrap samples, each as long as the original data sequence. To do resampling, we initialise values for $x_1$ and $x_2$ by picking a random consecutive sequence from the data. Then, we use the provided function `ARp.filter()` to generate a new sample based on the bootstrapped residuals. Finally, we regress the new time-series to obtain bootstrapped estimates of the different $\boldsymbol \beta$'s.
```{r, echo = T, eval = T}
set.seed(4300)
# Number of Bootstrap Samples
B = 1500

# Bootstrap the residuals B times
e.bootstrapped = matrix(sample(e.observed.LS, size=B*(n-2), replace=T), nrow = B, ncol = n-2)

# Create a random consecutive sequence to initialise
index = sample(99, B, replace=T)
index.mat = matrix(c(index,index+1),nrow = B,ncol = 2)
x0 = matrix(data3A$x[index.mat], nrow = B, ncol = 2)

# Prepare to allocate data
bootstrapped.LS = matrix(NA, nrow = B, ncol = 2)
bootstrapped.LA = matrix(NA, nrow = B, ncol = 2)
# Prepare to allocate data to create residuals from the B different pairs of beta
e.bootstrapped.beta.LS = matrix(NA, nrow = B, ncol = n-2)
e.bootstrapped.beta.LA = matrix(NA, nrow = B, ncol = n-2)

for (i in 1:B){
  # Create time-series
  bootstrapped.sample.LS = ARp.filter(x0[i,], beta.LS, e.bootstrapped[i,])
  bootstrapped.sample.LA = ARp.filter(x0[i,], beta.LA, e.bootstrapped[i,])
  
  # Regress on the time-series
  bootstrapped.LS[i,] = ARp.beta.est(bootstrapped.sample.LS, 2)$LS
  bootstrapped.LA[i,] = ARp.beta.est(bootstrapped.sample.LA, 2)$LA
  
  # Compute the corresponding residuals based on the created time-series and the computed betas
  e.bootstrapped.beta.LS[i,] = ARp.resid(bootstrapped.sample.LS,bootstrapped.LS[i,])
  e.bootstrapped.beta.LA[i,] = ARp.resid(bootstrapped.sample.LA,bootstrapped.LA[i,])
}
```

We use the bootstrap to obtain variance and bias of the two estimators. To estimate the bias, we use the plug-in principle, and define the estimate of the bias as
$$\text{bias}_{\hat{F}_0} = E_{\hat{F}_0} [s(\mathbf{x}^\ast)]-\hat{\boldsymbol \beta},$$
where $\hat{F}_0$ is the bootstrap sample distribution, $\mathbf{x}^\ast$ is a bootstrap sample and $s(\cdot)$ is the bootstrap estimator. To estimate the variance, we use the sample variance of the bootstrap estimators.

```{r, echo = T, eval = F}
# Mean of bootstrap estimators
bootstrap.estimate.LS = apply(bootstrapped.LS,2,mean)
bootstrap.estimate.LA = apply(bootstrapped.LA,2,mean)

# Variance of bootstrap estimators
bootstrap.variance.LS = apply(bootstrapped.LS,2,var)
bootstrap.variance.LA = apply(bootstrapped.LA,2,var)
cat("Bias of beta.LS: ", bootstrap.estimate.LS - beta.LS, "\n")
cat("Bias of beta.LA: ", bootstrap.estimate.LA - beta.LA, "\n")

cat("Variance of beta.LS: ", bootstrap.variance.LS, "\n")
cat("Variance of beta.LA: ", bootstrap.variance.LA, "\n")
```
```{r, echo = F, eval = T}
# Mean of bootstrap estimators
bootstrap.estimate.LS = apply(bootstrapped.LS,2,mean)
bootstrap.estimate.LA = apply(bootstrapped.LA,2,mean)

# Variance of bootstrap estimators
bootstrap.variance.LS = apply(bootstrapped.LS,2,var)
bootstrap.variance.LA = apply(bootstrapped.LA,2,var)
cat("Bias of beta.LS: ", bootstrap.estimate.LS - beta.LS, "\n")
cat("Bias of beta.LA: ", bootstrap.estimate.LA - beta.LA, "\n")

cat("Variance of beta.LS: ", bootstrap.variance.LS, "\n")
cat("Variance of beta.LA: ", bootstrap.variance.LA, "\n")
```

We know that for a Gaussian $AR(p)$ process, the LS estimator will be optimal. However, from the print-out above, we see that the LA estimator both has smaller bias and smaller variance than the LS estimator. Our data is not Gaussian, which explains why the LA estimator is optimal and not the LS estimator.


## 2.
We now want to compute a $95\%$ prediction interval for $x_{101}$ based on both the LS and the LA estimator. We use the bootstrapped time-series and the 1500 estimates for $\boldsymbol \beta$ obtained in part 1. to estimate the residual distribution, then we use the following formula to simulate a value $x_{101}$ for the observed time-series
$$x_{101} = \beta_1 x_{99} + \beta_2 x_{100} + e_{101},$$
where $e_{101}$ is drawn from the residual distribution. By doing this, the simulated $x_{101}$ values will reflect both our lack of knowledge about the parameter values, and our lack of knowledge about the residual distribution. We then display histograms of the simulated distributions of $x_{101}$, and also find limits in the prediction interval as quantiles in the simulated values.

```{r, echo = T, eval = F, out.width = "60%"}
set.seed(4300)
# Create a vector of the values of x_99 and x_100
x = data3A$x[99:100]

# x_101 = beta_1 * x_99 + beta_2 * x_100 + residual
x.101.LS = as.vector(bootstrapped.LS%*%x)
x.101.LA = as.vector(bootstrapped.LA%*%x)
for (i in 1:B){
  x.101.LS[i] = x.101.LS[i] + sample(e.bootstrapped.beta.LS[i,], size = 1)
  x.101.LA[i] = x.101.LA[i] + sample(e.bootstrapped.beta.LA[i,], size = 1)
}

df.LS = data.frame(x = x.101.LS)
p1 <- ggplot(df.LS, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LS estimators") + xlab("x_101") + theme_grey(base_size = 8)
df.LA = data.frame(x = x.101.LA)
p2 <- ggplot(df.LA, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LA estimators") + xlab("x_101") + theme_grey(base_size = 8)
grid.arrange(p1,p2, nrow = 1)
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```
```{r, echo = F, eval = T, out.width='60%', fig.cap="\\label{fig:origest}Histogram of $x_{101}$ based on LS and LA estimates of $\\boldsymbol{\\beta}$ and residuals based on each bootstrapped time-series.",fig.align = "center"}
set.seed(4300)
# Create a vector of the values of x_99 and x_100
x = data3A$x[99:100]

# x_101 = beta_1 * x_99 + beta_2 * x_100 + residual
x.101.LS = as.vector(bootstrapped.LS%*%x)
x.101.LA = as.vector(bootstrapped.LA%*%x)
for (i in 1:B){
  x.101.LS[i] = x.101.LS[i] + sample(e.bootstrapped.beta.LS[i,], size = 1)
  x.101.LA[i] = x.101.LA[i] + sample(e.bootstrapped.beta.LA[i,], size = 1)
}

df.LS = data.frame(x = x.101.LS)
p1 <- ggplot(df.LS, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LS estimators") + xlab("x_101") + theme_grey(base_size = 8)
df.LA = data.frame(x = x.101.LA)
p2 <- ggplot(df.LA, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LA estimators") + xlab("x_101") + theme_grey(base_size = 8)
grid.arrange(p1,p2, nrow = 1)
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```

LEGGE TIL LINJER FOR 95% PRED INT?

We observe that the histograms from the two different estimation methods look very similar. However, the limits of the $95\%$ prediction interval for the LA estimator are somewhat higher than the interval for the LS estimator. Still, this result could change if we set a different seed before sampling the residuals or before the bootstrap sampling.

If we study the histograms, we also see that we get some really large simulated values for $x_{101}$. However, these large values do not appear frequent enough to appear in the $95\%$ prediction intervals. The reason why these large values appear becomes evident if we plot a histogram of the residual distribution. This is done for the distribution based on LS estimates in figure \ref{fig:resdist}, and the distribution based on LA estimates will be very similar.

```{r, echo = T, eval = T, out.width = "60%", fig.align = "center", fig.cap="Histogram of residual distribution based on bootstrapped LS estimates of $\\boldsymbol{\\beta}$ and original LS estimate of $\\boldsymbol{\\beta}$. \\label{fig:resdist}"}
df.e = data.frame(x= as.vector(e.bootstrapped.beta.LS))
df.e.b = data.frame(x = e.observed.LS)
p3 <- ggplot(df.e, aes(x = x)) + geom_histogram(aes(y = ..density..), bins = 100) + ggtitle("Histogram of residual distribution based \n on bootstrapped LS estimates of beta") + xlab("residual") + theme_grey(base_size = 8)
p4 <- ggplot(df.e.b, aes(x = x)) + geom_histogram(aes(y = ..density..), bins = 100) + ggtitle("Histogram of residual distribution based \n on original LS estimate of beta") + xlab("residual") + theme_grey(base_size = 8)
grid.arrange(p3,p4, nrow = 1)
```
From figure \ref{fig:resdist}, we see that some residuals have a value of $\simeq 30$, while the rest are concentrated in the range $[-10,10]$. This explains the observed large values for $x_{101}$.


We also observe that the residuals from the original LS estimate has almost the same distribution as the residuals from the bootstrapped LS estimates. Thus, we propose a new method to estimate the distribution of $x_{101}$: We use the bootstrapped values of $\boldsymbol \beta$ and simulate the residuals from the initial observed residuals achieved wihtout bootstrapping. This is done below.


```{r, echo = T, eval = F}
set.seed(4300)
residuals = sample(e.observed.LS, size = B, replace = T)
x.101.LS = as.vector(bootstrapped.LS%*%x) + residuals
x.101.LA = as.vector(bootstrapped.LA%*%x) + residuals
df.bs.LS = data.frame(x = x.101.LS)
p5 <- ggplot(df.bs.LS, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LS estimators") + xlab("x_101") + theme_grey(base_size = 8)
df.bs.LA = data.frame(x = x.101.LA)
p6 <- ggplot(df.bs.LA, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LA estimators") + xlab("x_101") + theme_grey(base_size = 8)
grid.arrange(p5,p6, nrow = 1)
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```
```{r, echo = F, eval = T, out.width='60%', fig.cap="\\label{fig:origest}Histogram of $x_{101}$ based on LS and LA estimates of $\\boldsymbol{\\beta}$ and residuals based on the original time-series.",fig.align = "center"}
set.seed(4300)
residuals = sample(e.observed.LS, size = B, replace = T)
x.101.LS = as.vector(bootstrapped.LS%*%x) + residuals
x.101.LA = as.vector(bootstrapped.LA%*%x) + residuals
df.bs.LS = data.frame(x = x.101.LS)
p5 <- ggplot(df.bs.LS, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LS estimators") + xlab("x_101") + theme_grey(base_size = 8)
df.bs.LA = data.frame(x = x.101.LA)
p6 <- ggplot(df.bs.LA, aes(x = x)) + geom_histogram(aes(y = ..density..)) + ggtitle("Histogram of simulated distribution of \n x_101 based on LA estimators") + xlab("x_101") + theme_grey(base_size = 8)
grid.arrange(p5,p6, nrow = 1)
cat("Limits of 95% prediction interval for LS estimator: ", c(sort(x.101.LS)[round(B*0.025)], sort(x.101.LS)[round(B*0.975)]),"\n")
cat("Limits of 95% prediction interval for LA estimator: ", c(sort(x.101.LA)[round(B*0.025)], sort(x.101.LA)[round(B*0.975)]))
```
We see that both the prediction interval values, and the histograms achieved from this method are very similar to the first method. This result is as expected, as the residuals should have the same distribution due to the nature of bootstrapping. The bootstrapped time-series are created by bootstrapping the observed residuals, and the time-series are then used to create the residuals used in the first method.

# Problem B: Permutation test

## 1)

We wish to inspect the concentrations of bilirubin in the blood of three young men for possible evidence of health issues. We first inspect the logarithms of the measured concentrations for each individual.
```{r, echo=T, eval=T, out.width="50%", fig.align="center", fig.cap="Box plot of the logarithm of the measured concentration (mg/dL) in blood samples taken from three young men. \\label{boxplot}"}
bilirubin <- read.table("bilirubin.txt",header=T)
bilirubin['log_meas'] = log(bilirubin$meas)
ggplot(data = bilirubin, aes(x = pers, y = log_meas)) + geom_boxplot(outlier.colour="black", outlier.shape=16,
             outlier.size=2, notch=FALSE) + theme_grey()
```
We observe from figure \ref{boxplot} that the median (the middle bold line) for person 1 and person 2 are very close to eachother and far from the median for person 3. From the 1st and 3rd quartiles of the boxplots (indicated by the lower and upper hinges) we observe that there is little spread in the measurements for person 2, some spread for person 3 and a lot of spread for person 1. The whiskers denote the largest / smallest measurement that is whithin $1.5 * IQR$ (the range between the 1st and 3rd quantile) from the upper / lower hinge. This measures the total spread in the data. For person 1 and person 2 we see that the whiskers extend relatively far into the range of the boxplot of person 3. The medians indicate a large difference between the bilirubin levels of person 1 / 2 and person 3 but observing (from the whiskers) the large spread in the data we should also investigate this significance quantitatively.

We fit a linear model $$\log Y_{ij} \sim \beta_{p_j} + e_{ij}, \quad e_{ij} \overset{iid}{\sim} N(0, \sigma^2)$$

```{r}
model0 = lm(log(meas)~pers,data=bilirubin)
summary_model0 = summary(model0)
fstatistic_model0 = summary_model0$fstatistic[1]

summary_model0
```
From the summary we can read that, as expected, the coefficient for `persp3` is large. We can also read that the hypothesis 

$$ H_0: \beta_{p_1} = \beta_{p_2} = \beta_{p_3} \quad \text{vs} \quad  H_1: \beta_{p_1} \neq \beta_{p_2} \lor \beta_{p_1} \neq \beta_{p_3} \lor \beta_{p_2} \neq \beta_{p_3}$$
attains the F-value `r round(as.numeric(fstatistic_model0), 4)` with a corresponding p-value of `r round(1 - pf(fstatistic_model0, summary_model0$fstatistic[2], summary_model0$fstatistic[3]), 4)`, which is below any reasonable significance level. We note that this F-statistic is an estimator $\theta(x)$ and denote this original value of the estimator  $\hat{F}_0$. 

Based on the p-value corresponding to $\hat{F}_0$ we may be tempted to already conclude that the total regression is signficant and thus that person 3 is accumulating bilirubin in his blood. However, we now wish to investigate the distribution of $\hat{F}$ under $H_0$.

## 2)

We wish to investigate how, under $H_0$, likely it is to observe something equally or more extreme than $\hat{F}_0$. Assuming first $H_0$ to be true, we can argue that observing a permutation of the observed measurements would be equally likely as observing our original observations. We may therefore shuffle the measurements between the different men and compute a new F_statistic $\hat{F}^{(1)}_{H_0}$. We reshuffle $999$ times and compute $\{\hat{F}^{(1)}_{H_0}, \hat{F}^{(2)}_{H_0}, \dots, \hat{F}^{(999)}_{H_0}\}$.

```{r}
set.seed(4300)
fstatistic_length = 999
fstatistic_values = c(length.out = fstatistic_length)

permTest <- function() {
  dataframe = data.frame(bilirubin)
  dataframe$pers = sample(bilirubin$pers, size=length(bilirubin$pers), replace = FALSE)
  sum = summary(model0)
  return(summary(lm(log(meas)~pers,data=dataframe))$fstatistic['value'])
}
```


## 3)
We may use these $999$ samples as an estimate of the distribution of $\hat{F}$ under $H_0$. 

```{r, echo=T, eval=T, out.width="50%", fig.align="center", fig.cap="Histogram of the samples $\\{\\hat{F}^{(i)}_{H_0}\\}$. The lines indicate the $95\\%$ quantile, the value of the original estimate $\\hat{F}_0$ and the Fisher-distribution corresponding to the degrees of freedom in the regression. \\label{fig:histogram}"}

# Permute observations
for (i in seq(0, fstatistic_length)) {
  fstatistic_values[i] = permTest()
}


df_fstatistic_values = data.frame(x = fstatistic_values)
fstatistic_quantiles = quantile(fstatistic_values, probs = c(0.95))
ggplot() + geom_histogram(data = df_fstatistic_values, aes(x = x, y = ..density.., col="permuted"), bins=100) + stat_function(data = data.frame(x = c(0, 6)), lwd=1., aes(x, col=paste("F", summary_model0$fstatistic[2],"," ,summary_model0$fstatistic[3],"-distribution", sep="")), fun = df, args = list(df1 = summary_model0$fstatistic[2], df2 = summary_model0$fstatistic[3])) + geom_vline(aes(xintercept = c(fstatistic_quantiles), col="95% quantile"), lwd=1.) + geom_vline(aes(xintercept = fstatistic_model0, col="original"), lwd=1.) + xlim(0, 6) + theme_grey()
```


We observe from figure \ref{fig:histogram} that under $H_0$ the majority of the estimates fall below that of the original value of $\hat{F}_0$, but we also observe some larger estimates. From the $95\%$ quantile, we note that $\hat{F}_0$ is larger than at least $95\%$ of the estimates $\{\hat{F}^{(i)}_{H_0}\}$. To be more exact we can estimate the p-value of $\hat{F}$ under $H_0$, i.e. the probability of observing a value equally or more extreme than $\hat{F}_0$ by counting the number of observations $\{\hat{F}^{(i)}_{H_0}\}$ that are larger then $\hat{F}_0$. 

```{r}
index = which.min(abs(sort(fstatistic_values) - fstatistic_model0))
cat("p-value: ", (fstatistic_length - index)/fstatistic_length)
```

Here we observe an estimate of the p-value to be `r (fstatistic_length - index)/fstatistic_length`, which corresponds well with the original p-value of `r 1 - pf(fstatistic_model0, summary_model0$fstatistic[2], summary_model0$fstatistic[3])`. We could have foreseen that these p-values would be similar by observing, from figure \ref{fig:histogram}, that the distribution of $\{\hat{F}^{(i)}_{H_0}\}$ is very similar to the corresponding Fisher distribution. This gives us reason to believe that the assumptions of the F-test, i.e. that the regression errors are iid. normal, are satisfied and that F-test will yield a correct p-value. 

Regardless of our choice of p-value we can argue that the total regression is significant and reject $H_0$ on a $0.05$ significance level. This gives us reason to believe that person 3 is accumulating bilirubin in his blood.

# Problem C: The EM-algorithm and bootstrapping

We let $\{x_i\}_{i=1,\dots,n}$ be i.i.d. $\text{Exp}(\lambda_0)$ and $\{y_i\}_{i=1,\dots,n}$ be i.i.d. $\text{Exp}(\lambda_1)$. We assume that we observe neither $x_i$ nor $y_i$ directly, but rather observe

$$z_i = \max(x_i, y_i), \quad \text{and} \quad u_i = I(x_i \geq y_i), \quad i = 1, \dots, n,$$
where $I(\cdot)$ is the indicator function. This means that we only observe the largest of the pair $(x_i, y_i)$ and we know whether the observed value is $x_i$ or $y_i$.

Based on the observed $(z_i,u_i) \quad i= 1,\dots,n$ we want use the Expecation-Maximization algorithm to find the maximum likelihood estimates for $(\lambda_0,\lambda_1)$.

## 1.

We first formulate the conditional probability of the complete data $\mathbf{x} = (x_1,\dots,x_n)$ and $\mathbf{y} = (y_1,\dots,y_n)$ given $(\lambda_0, \lambda_1)$. 

$$ f_{X_i, Y_i} (x_i, y_i \lvert \lambda_0, \lambda_1) = f_{X_i} (x_i \lvert \lambda_0) f_{Y_i} (y_i \lvert \lambda_1)= \lambda_0 e^{-\lambda_0 x_i } \lambda_1 e^{-\lambda_1 y_i },$$
because $x_i$ and $y_i$ are assumed independent. This gives
$$f(\mathbf{x}, \mathbf{y} \lvert \lambda_0, \lambda_1) = \prod_{i=1}^n f_{X_i, Y_i} (x_i, y_i \lvert \lambda_0, \lambda_1) = (\lambda_0 \lambda_1)^n \exp \big (-\lambda_0 \sum_{i=1}^n x_i \big )\exp \big (-\lambda_1 \sum_{i=1}^n y_i \big ),$$
and finally the loglikelihood
$$ \ln f(\mathbf{x}, \mathbf{y} \lvert \lambda_0, \lambda_1) = n (\ln \lambda_0 + \ln \lambda_1) -\lambda_0 \sum_{i=1}^n x_i   -\lambda_1 \sum_{i=1}^n y_i.$$

We now find the conditional probabilities of $x_i$ and $y_i$ given $z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}$

$$f_{X_i} (x_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}) = \begin{cases}\frac{\lambda_0^{(t)} \exp \{ -\lambda_0^{(t)} x_i \}}{1 - \exp \{-\lambda_0^{(t)} z_i \}}, \quad &u_i = 0, \\
z_i, \quad &u_i = 1,
\end{cases}$$

$$f_{Y_i} (y_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}) = \begin{cases}z_i, \quad &u_i = 0, \\
\frac{\lambda_1^{(t)} \exp \{ -\lambda_1^{(t)} y_i \}}{1 - \exp \{-\lambda_1^{(t)} z_i \}}, \quad &u_i = 1.
\end{cases}$$

Then we use these to formulate the expectation

$$E_{\mathbf{x}, \mathbf{y}} [ \ln f(\mathbf{x}, \mathbf{y} \lvert \lambda_0, \lambda_1) \lvert \mathbf{z}, \mathbf{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] 
\\=  n (\ln \lambda_0 + \ln \lambda_1) -\lambda_0 \sum_{i=1}^nE[x_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)} ]   - \lambda_1 \sum_{i=1}^n E[y_i, \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}],
$$

$$ E[x_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}] = u_i z_i + (1-u_i) \int_0^{z_i} x_i \frac{\lambda_0^{(t)} \exp \{ -\lambda_0^{(t)} x_i \}}{1 - \exp \{-\lambda_0^{(t)} z_i \}} dx_i 
\\ = u_i z_i + (1 - u_i) \bigg (\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp{\lambda_0^{(t)} z_i} - 1} \bigg),
$$

$$ E[y_i \lvert z_i, u_i, \lambda_0^{(t)}, \lambda_1^{(t)}] = (1-u_i) z_i + u_i \int_0^{z_i} y_i \frac{\lambda_1^{(t)} \exp \{ -\lambda_1^{(t)} y_i \}}{1 - \exp \{-\lambda_1^{(t)} z_i \}} dy_i 
\\ = (1-u_i) z_i + u_i \bigg (\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp{\lambda_1^{(t)} z_i} - 1} \bigg),$$

yielding,

$$\begin{aligned} 
Q(\lambda_0, \lambda_1 \lvert \lambda_0^{(t)}, \lambda_1^{(t)}) = E_{\mathbf{x}, \mathbf{y}} [ \ln f(\mathbf{x}, \mathbf{y} \lvert \lambda_0, \lambda_1) \lvert \mathbf{z}, \mathbf{u}, \lambda_0^{(t)}, \lambda_1^{(t)}] 
&=  n (\ln \lambda_0 + \ln \lambda_1) \\ 
&-\lambda_0 \sum_{i=1}^n u_i z_i + (1 - u_i) \bigg (\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp{\lambda_0^{(t)} z_i} - 1} \bigg) \\
&- \lambda_1 \sum_{i=1}^n  (1-u_i) z_i + u_i \bigg (\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp{\lambda_1^{(t)} z_i} - 1} \bigg).
\end{aligned}
$$

## 2.

This function $Q(\lambda_0, \lambda_1 \lvert \lambda_0^{(t)}, \lambda_1^{(t)})$ admits a maximum, which we can find using

$$\frac{\partial }{\partial \lambda_0} Q= 0 \quad\quad \frac{\partial }{\partial \lambda_1}Q= 0
$$

Giving 
\begin{align*} \lambda_0 &= \frac{n}{\sum_{i=1}^n u_i z_i + (1 - u_i) \bigg (\frac{1}{\lambda_0^{(t)}} - \frac{z_i}{\exp{\lambda_0^{(t)} z_i} - 1} \bigg)},
\\ \lambda_1 &= \frac{n}{\sum_{i=1}^n  (1-u_i) z_i + u_i \bigg (\frac{1}{\lambda_1^{(t)}} - \frac{z_i}{\exp{\lambda_1^{(t)} z_i} - 1} \bigg)},
\end{align*}

which gives us the recursion $(\lambda_0^{t}, \lambda_1^{t}) \mapsto (\lambda_0^{(t+1)}, \lambda_1^{(t+1)})$. We implement this recursion to 

```{r, echo=T, eval=T, out.width="50%", fig.align="center", fig.cap="Convergence plot for $(\\lambda_0, \\lambda_1)$ for finding the maximum likelihood estimates using recursion. \\label{fig:conv_lamb}"}
u = read.table("u.txt")[,1]
z = read.table("z.txt")[,1]
n = length(u)

n.iterations = 30
lambda = array(dim=c(n.iterations, 2))
lambda[1,] = c(1, 6)


for(i in 2:n.iterations){
  lambda[i, 1] = n / sum(u*z + (1-u)*(1/lambda[i-1, 1] - z / (exp(lambda[i-1, 1]*z - 1))))
  lambda[i, 2] = n / sum((1-u)*z + u*(1/lambda[i-1, 2] - z / (exp(lambda[i-1, 2]*z - 1))))
}

lambda_orig = lambda[n.iterations,]


df = data.frame(n = 1:n.iterations, lambda)

ggplot() + geom_line(data = df, aes(x = n, y = X1, col="lambda_0")) + geom_line(data = df, aes(x = n, y = X2, col="lambda_1")) + ylab("") + ggtitle("Convergence plot for lambda_0 and lambda_1") + theme_grey()
```

## 3).

```{r, echo=T, eval=T, out.width="80%", fig.align="center", fig.cap="The distribution of the bootstrap samples of the EM-estimates of $(\\lambda_0, \\lambda_1)$. \\label{fig:bs_lamb}"}
algorithmEM = function(u,z){
  lambda = 5*runif(2)
  lambda_old = c(-1, -1)
  while(sum((lambda - lambda_old)^2)>1e-12){
    lambda_old = lambda
    lambda[1] = n / sum(u*z + (1-u)*(1/lambda[1] - z / (exp(lambda[1]*z - 1))))
    lambda[2] = n / sum((1-u)*z + u*(1/lambda[2] - z / (exp(lambda[2]*z - 1))))
  }
  return(lambda)
}

n_bs = 20000
lambda_bs = array(dim = c(n_bs, 2))

for (i in 1:n_bs){
  index = sample(1:n, replace=T)
  u_bs = u[index]
  z_bs = z[index]
  lambda_bs[i,] = algorithmEM(u_bs, z_bs)
}

dfbs1 = data.frame(idx = "lambda_0", x = lambda_bs[, 1], x0 = lambda_orig[1], mean = mean(lambda_bs[, 1]))
dfbs2 = data.frame(idx = "lambda_1", x = lambda_bs[, 2], x0 = lambda_orig[2], mean = mean(lambda_bs[, 2]))
df_bs = rbind(dfbs1, dfbs2)

ggplot() + geom_histogram(data = df_bs, aes(x = x, y = ..density.., col="bootstrap samples"), bins=100) + geom_vline(aes(xintercept = x0, col="original MLE"), data = df_bs, lwd=1.0) +
geom_vline(aes(xintercept = mean, col="bootstrap mean"), data = df_bs, lwd=1.0) + xlim(0,16) + facet_wrap(~idx) + theme_grey()
```
```{r, echo=T, eval=T}
bias = apply(lambda_bs,2,mean) - lambda_orig
cat("Bias of the bootstrapped samples: ", bias, "\n")
cov(lambda_bs)
cor(lambda_bs)
```

SKRIV NOE HER

## 4).

As an alternative to this EM approach we can find the MLE by classical optimization techniques. We first find the density of $z_i$ as the maximum of the two exponentially distributed variables $x_i$ and $y_i$. 

\begin{align*}
P(z_i \leq z) &= P(x_i \leq z_i) P(y_i \leq z_i) = (1 - e^{-\lambda_0 z_i}) (1 - e^{-\lambda_1 z_i}) \\
F_{Z_i}(z_i) &= (1 - e^{-\lambda_0 z_i}) (1 - e^{-\lambda_1 z_i})
\end{align*}
We derive to get the pdf of $z_i$,

$$ f_{Z_i}(z_i) = F_{Z_i}'(z_i) =  \lambda_0 e^{-\lambda_0 z_i} + \lambda_1 e^{-\lambda_1 z_i} - (\lambda_0 + \lambda_1) e^{-(\lambda_0 + \lambda_1)z_i}.$$

If we condition on the value of $z_i$ we get that $u_i$ is Bernoulli-distributed with probabilites 
$$p(u_i = 1 \lvert z_i) = \frac{f_{X_i}(z_i)}{f_{X_i}(z_i) + f_{Y_i}(z_i)} \quad\text{and}\quad p(u_i = 0 \lvert z_i) = \frac{f_{Y_i}(z_i)}{f_{X_i}(z_i) + f_{Y_i}(z_i)}$$

We combine these results to get the joint of $(z_i, u_i)$

$$f_{Z_i, U_i}(z_i, u_i) = (\lambda_0 e^{-\lambda_0 z_i} + \lambda_1 e^{-\lambda_1 z_i} - (\lambda_0 + \lambda_1) e^{-(\lambda_0 + \lambda_1)z_i})\cdot  \begin{cases} \frac{\lambda_0 e^{-\lambda_0 z_i}}{\lambda_0 e^{-\lambda_0 z_i} + \lambda_1 e^{-\lambda_1 z_i}} \quad \quad u_i = 0 \\
\frac{\lambda_1 e^{-\lambda_1 z_i}}{\lambda_0 e^{-\lambda_0 z_i} + \lambda_1 e^{-\lambda_1 z_i}} \quad \quad u_i = 1 
\end{cases},$$

and formulate the log-likelihood of the observations $(\mathbf{z}, \mathbf{u})$

\begin{equation}
\begin{split}
\log f_{\mathbf{Z}, \mathbf{U}}(\mathbf{z}, \mathbf{u}) 
&= \sum_{i=1}^n \log \Big(\lambda_0 e^{-\lambda_0 z_i} + \lambda_1 e^{-\lambda_1 z_i} - (\lambda_0 + \lambda_1) e^{-(\lambda_0 + \lambda_1)z_i} \Big) \\
&- \sum_{u_i = 0} \log \Big( 1 + \frac{\lambda_1}{\lambda_0} e^{- (\lambda_1 - \lambda_0) z_i} \Big ) \\
&- \sum_{u_i = 1} \log \Big( 1 + \frac{\lambda_0}{\lambda_1} e^{-(\lambda_0 - \lambda_1) z_i}\Big).
\end{split}
\label{eq:ll}
\end{equation}

Classically, we would want to take the partial derivative with respect to $\lambda_0$,

\begin{align*} \frac{\partial \log f_{\mathbf{Z}, \mathbf{U}}(\mathbf{z}, \mathbf{u})}{\partial \lambda_0} 
&= \sum_{i=1}^n \frac{e^{-\lambda_0 z_i}(1 - \lambda_0 z_i) - e^{-(\lambda_0 + \lambda_1)z_i}(1-z_i(\lambda_0 + \lambda_1))}{\lambda_0 e^{-\lambda_0 z_i} + \lambda_1 e^{-\lambda_1 z_i} - (\lambda_0 + \lambda_1) e^{-(\lambda_0 + \lambda_1)z_i} } 
\\
&+ \sum_{u_i = 0} \frac{(\frac{1}{\lambda_0} - z_i)\lambda_1 e^{-\lambda_1 z_i}}{\lambda_0 e^{-\lambda_0 z_i} + \lambda_1 e^{-\lambda_1 z_i}}
\\
&+ \sum_{u_i = 1} \frac{-(1 - z_i\lambda_0) e^{-\lambda_0 z_i}}{\lambda_0 e^{-\lambda_0 z_i} + \lambda_1 e^{-\lambda_1 z_i}},
\end{align*}

and with respect to $\lambda_1$,

\begin{align*} \frac{\partial \log f_{\mathbf{Z}, \mathbf{U}}(\mathbf{z}, \mathbf{u})}{\partial \lambda_1} 
&= \sum_{i=1}^n \frac{e^{-\lambda_1 z_i}(1 - \lambda_1 z_i) - e^{-(\lambda_0 + \lambda_1)z_i}(1-z_i(\lambda_0 + \lambda_1))}{\lambda_0 e^{-\lambda_0 z_i} + \lambda_1 e^{-\lambda_1 z_i} - (\lambda_0 + \lambda_1) e^{-(\lambda_0 + \lambda_1)z_i} } 
\\
&+ \sum_{u_i = 0} \frac{-(1 - z_i\lambda_1) e^{-\lambda_1 z_i}}{\lambda_0 e^{-\lambda_0 z_i} + \lambda_1 e^{-\lambda_1 z_i}}
\\
&+ \sum_{u_i = 1} \frac{(\frac{1}{\lambda_1} - z_i)\lambda_0 e^{-\lambda_0 z_i}}{\lambda_0 e^{-\lambda_0 z_i} + \lambda_1 e^{-\lambda_1 z_i}}.
\end{align*}

and find the maximum-likelhood parameters $(\lambda_0, \lambda_1)$ by solving
$$ \frac{\partial \log f_{\mathbf{Z}, \mathbf{U}}(\mathbf{z}, \mathbf{u})}{\partial \lambda_0}, \quad \text{and} \quad \frac{\partial \log f_{\mathbf{Z}, \mathbf{U}}(\mathbf{z}, \mathbf{u})}{\partial \lambda_1} = 0.$$

However, we here choose to optimize the loglikelihood  given in equation \ref{eq:ll} using the derivative-free optimization technique implemented in R's `optim` function. This is an implementation of the Nelder-Mead algorithm.

```{r, echo=T, eval=T, out.width="50%", fig.align="center", fig.cap="Contour-plot of the log-likelihood defined above as well as its optimum, the original EM-estimate of $(\\lambda_0, \\lambda_1)$ and the bootstrap mean of $(\\lambda_0, \\lambda_1)$. \\label{fig:llik_lamb}"}

#Loglikelihood as defined above
loglik = function(x){
  lambda_0 = x[1]
  lambda_1 = x[2]
  # 1st sum
  res = sum(log(lambda_0 * exp(-lambda_0 * z) + lambda_1 * exp(-lambda_1 * z) - (lambda_0 + lambda_1)*exp(-(lambda_0 + lambda_1)*z)))
  # 2nd sum
  res = res - sum(u*log(1 + lambda_1 / lambda_0 * exp(-(lambda_1 - lambda_0)*z)))
  # 3rd sum
  res = res - sum((1-u)*log(1 + lambda_0 / lambda_1 * exp(-(lambda_0 - lambda_1)*z)))
  return(res)
}

# Find optimal lambda using derivative-free optimization with initial guess of (4, 12)
lambda_optim = optim(par=c(4, 12), fn=loglik, control = list(fnscale = -1, maxit=1000, reltol=1e-16, ndeps=1e-6))$par

# Define area as [1, 7] x [1, 15]
x0 = seq(1, 7, length.out = 30)
x1 = seq(1, 15, length.out = 30)
out = array(dim = c(30, 30, 3))

# Compute loglikelihood over area
for (i in 1:30){
  for (j in 1:30){
    out[i, j, 1] = x0[i]
    out[i, j, 2] = x1[j]
    out[i, j, 3] = loglik(c(x0[i], x1[j]))
  }
}

dfll = data.frame(lambda_0 = as.vector(out[,, 1]), lambda_1 = as.vector(out[,,2]), llik = as.vector(out[,,3]))
dfll_dot1 = data.frame(lambda_0 = lambda_orig[1], lambda_1 = lambda_orig[2])
dfll_dot3 = data.frame(lambda_0 = mean(lambda_bs[, 1]), lambda_1 = mean(lambda_bs[, 2]))
dfll_dot2 = data.frame(lambda_0 = lambda_optim[1], lambda_1 = lambda_optim[2])

ggplot() + geom_contour(data = dfll, aes(x = lambda_0, y = lambda_1, z = llik), binwidth=10, show.legend=T)  + geom_point(data = dfll_dot1, aes(x = lambda_0, y = lambda_1, col="EM-estimate"), size=2) + geom_point(data = dfll_dot2, aes(x = lambda_0, y = lambda_1, col="optimized"), size=2) + geom_point(data = dfll_dot3, aes(x = lambda_0, y = lambda_1, col="bootstrap mean"), size=2)
```

We see from figure \ref{fig:llik_lamb} that the optimization scheme correctly finds the MLE $(\hat{\lambda}_0, \hat{\lambda}_1)$ of $(\lambda_0, \lambda_1)$. We also see that there is some difference between the EM estimates (bootstrap and original) and the estimate found by optimizing the loglikelihood.

```{r, echo=T, eval=T}
cat("EM:", lambda_orig, "\n")
cat("MLE", lambda_optim)
```

Especially in $\lambda_1$ we see a difference. If we compare the log-likelihood of the two estimates

```{r, echo=T, eval=T}
cat("loglik EM:", loglik(lambda_orig), "\n")
cat("loglik MLE:", loglik(lambda_optim), "\n")
```

We see that there is very little difference in the log-likelihood. From the plot \ref{fig:llik_lamb} we see that in a neighbouring area of the MLE $(\hat{\lambda}_0, \hat{\lambda}_1)$ the slope of the log-likelihood is very weak. We suspect that, as a result, the convergence criteria of the EM-algorithm will be satisfied before it can converge to the MLE. 

We note that, in general, the linear convergence of EM can be slow compared to the convergence of classical optimization techniques. With the analytical likelihood, we can formulate the derivatives of the loglikelihood and rely on the well-developed theory of derivative-based optimization techniques. If we also formulate the hessian we can attain the quadratic convergence of the Newton's method. In practice we can then also use optimization frameworks such as R's `optim` function or Python's `scipy.linalg`.

Additionally, it will be difficult to formulate fitting convergence criteria for the EM algorithm. Typically the algorithm is set to terminate when the norm of the change from one iteration to the next reaches some lower value. However, this lower value must be chosen in accordance with the slope of the loglikelihood around the MLE. As we saw in this case, it is difficult to do this without first also formulating the analytical loglikelihood. 

However, the ease of implementation and the stable ascent of EM are often very attractive despite its slow convergence.
